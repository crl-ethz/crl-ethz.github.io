<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://crl-ethz.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://crl-ethz.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-28T11:50:55+00:00</updated><id>https://crl-ethz.github.io/feed.xml</id><title type="html">blank</title><subtitle>Available student projects in Computation Robotics Lab (CRL), ETH Zurich </subtitle><entry><title type="html">Physics-informed digital twin from video for task and motion planning of deformable objects</title><link href="https://crl-ethz.github.io/phystwin-video-deformable-tamp/" rel="alternate" type="text/html" title="Physics-informed digital twin from video for task and motion planning of deformable objects"/><published>2025-05-26T10:00:00+00:00</published><updated>2025-05-26T10:00:00+00:00</updated><id>https://crl-ethz.github.io/phystwin-video-deformable-tamp</id><content type="html" xml:base="https://crl-ethz.github.io/phystwin-video-deformable-tamp/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Achieving proficient robotic manipulation of deformable objects, such as ropes, textiles, and other flexible materials, remains a significant challenge in the robotics community. While recent advances in vision-language-action models [1] demonstrated remarkable capabilities in learning robust, long-horizon behaviors from demonstrations, they are extremely data-hungry and struggle with out-of-distribution tasks. Model-based task and motion planners, on the other hand, can generalize better to unseen tasks because of explicit reasoning at both task and motion levels.</p> <p>Effective task and motion planning (TAMP) for deformable objects heavily depends on accurate physics simulations to predict how objects respond to robot control commands. However, traditional model-based physics simulations often encounter significant sim-to-real gaps due to unmodeled or difficult-to-simulate physical behaviors and inaccuracies in system identification. Recent advancements in physics-informed digital twins, such as the PhysTwin framework proposed by [2], present a promising approach by enabling robots to quickly and autonomously construct realistic and interactive simulations of deformable objects from minimal video data. This method holds great potential to bridge the sim-to-real gap inherent in robotics planning.</p> <p>Our goal is to evaluate the potential of integrating PhysTwin into an optimization-based TAMP framework developed by the mentor. The TAMP framework plans robot control commands to manipulate a rope on a table from any starting configuration to a user-specified target configuration, and co-optimizes a sequence of inputs specifying (1) where on the rope to grasp and (2) how to manipulate after grasp is engaged (velocity of the end-effector motion). The current TAMP framework employs a model-based physics simulation to formulate system evolution rules as constraints. Our goal is to replace this with the PhysTwin-learned model (fig. 1).</p> <p>If successful, this implementation will set the stage for future work involving “plan-to-learn” strategies [3], where robots iteratively refine their “intuitive physics engine” [4] of a target object through autonomously planned, interactive exploration of unknown deformable objects.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025_phystwin-video-deformable-tamp-480.webp 480w,/assets/img/publication_preview/2025_phystwin-video-deformable-tamp-800.webp 800w,/assets/img/publication_preview/2025_phystwin-video-deformable-tamp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/publication_preview/2025_phystwin-video-deformable-tamp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Figure 1: Left: Learning a physics simulation model from interaction videos using PhysTwin [2]. Right: Integration of the learned PhysTwin model into a model-based task and motion planner to enable robotic manipulation of a deformable object (a rope).</em></p> <h1 id="tasks">Tasks</h1> <ul> <li>Re-implement PhysTwin using their <a href="https://github.com/Jianghanxiao/PhysTwin">publicly available codebase</a> and evaluate its performance and limitations.</li> <li>Integrate the PhysTwin-trained model into a TAMP framework to plan long-horizon manipulations for rope-like deformable objects, validated through simulations and testing on a real robot manipulator system.</li> <li>Document guidelines and insights regarding the practical utility and constraints of physics-informed digital twins in robotic manipulation tasks.</li> <li>Prepare a thorough written report.</li> <li>Present findings and contributions formally.</li> </ul> <h1 id="requirements">Requirements</h1> <ul> <li>Strong background in optimization techniques and numerical methods.</li> <li>Proficiency in Python (PhysTwin validation) and experience with Julia or C++ (TAMP integration).</li> <li>Basic knowledge in physics simulation and trajectory optimization. Students who have taken the Physically-Based Simulation in Computer Graphics and/or Computational Models of Motion courses at ETH are preferred but not required.</li> <li>Self-driven attitude and persistence to read and experiment with existing cutting-edge research code, and the ability to independently explore and solve problems.</li> </ul> <h1 id="remarks">Remarks</h1> <p>This master’s thesis project is limited to ETH Zurich students. The project is overseen by Prof. Dr. Stelian Coros and supervised by Dr. Yijiang Huang and Jimmy Envall. For further information or to apply for the thesis project, please send your CV, transcript, and a short motivation letter to: Yijiang (yijiang.huang@inf.ethz.ch), and Jimmy (jimmy.envall@inf.ethz.ch).</p> <h1 id="references">References</h1> <p>[1] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: A vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024.</p> <p>[2] Hanxiao Jiang, Hao-Yu Hsu, Kaifeng Zhang, Hsin-Ni Yu, Shenlong Wang, and Yunzhu Li. PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos, March 2025.</p> <p>[3] Nishanth Kumar, Tom Silver, Willie McClinton, Linfeng Zhao, Stephen Proulx, Tomas Lozano-Pérez, Leslie Pack Kaelbling, and Jennifer Barry. Practice Makes Perfect: Planning to Learn Skill Parameter Policies, February 2024.</p> <p>[4] Peter W Battaglia, Jessica B Hamrick, and Joshua B Tenenbaum. Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences, 110(45):18327–18332, 2013.</p>]]></content><author><name></name></author><category term="planning"/><category term="physics-informed-modeling,"/><category term="deformable-objects,"/><category term="task-and-motion-planning,"/><category term="simulation"/><summary type="html"><![CDATA[Integrating physics-informed digital twins learned from videos into optimization-based TAMP frameworks for manipulating deformable objects.]]></summary></entry><entry><title type="html">Strategic Humanoid Box Lifting</title><link href="https://crl-ethz.github.io/stra-humanoid-box-lifting/" rel="alternate" type="text/html" title="Strategic Humanoid Box Lifting"/><published>2025-05-23T10:00:00+00:00</published><updated>2025-05-23T10:00:00+00:00</updated><id>https://crl-ethz.github.io/stra-humanoid-box-lifting</id><content type="html" xml:base="https://crl-ethz.github.io/stra-humanoid-box-lifting/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Manipulating large, heavy, and unwieldy objects from the ground presents a significant challenge for humanoid robots, especially in the absence of handles or convenient grasp affordances. Unlike small object grasping, this task requires coordinated full-body motion, careful balance control, and a sequence of contact-rich interactions such as pivoting, push-sliding, and re-grasping (fig. 1-(1-5)). Humans intuitively manage such tasks by adjusting their posture, leveraging friction, and shifting contact modes, but this remains an underexplored frontier for robotic learning.</p> <p>This project focuses on endowing a humanoid robot, specifically <a href="https://www.pndbotics.com/humanoid">Adam from PNDbotics</a>, with the ability to lift a flat, wide, heavy object (e.g., an IKEA-style box or raw construction material like a wood panel) from the ground using a learned policy. For simplicity, Adam will be equipped with a simple, ball-shaped gripper instead of dexterous hands.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025_stra-humanoid-box-lifting-480.webp 480w,/assets/img/publication_preview/2025_stra-humanoid-box-lifting-800.webp 800w,/assets/img/publication_preview/2025_stra-humanoid-box-lifting-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/publication_preview/2025_stra-humanoid-box-lifting.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Figure 1: Example of a human lifting a large, heavy package from the ground through a sequence of whole-body coordinated actions: initial pivoting (2–3), grasp adjustment and reorientation (4), push-sliding against the floor (4–5), and finally establishing a stable, upright configuration for transport (5). Our objective is to develop a reinforcement learning policy for a humanoid robot to replicate and generalize such strategies using full-body motion and dual-arm interaction with simple ball-shaped end-effectors.</em></p> <h1 id="tasks">Tasks</h1> <ul> <li>Design a data collection protocol to record one or few high-quality whole-body human demonstrations that include pivoting, pushing, sliding, and lifting behaviors.</li> <li>Extract contact-relevant features from these demonstrations using motion capture, object tracking, and possibly tactile instrumentation.</li> <li>Convert demonstration data into robot-feasible trajectories through a retargeting and trajectory optimization pipeline (e.g., [1]).</li> <li>Train a policy via demonstration-guided reinforcement learning to replicate and generalize the demonstrated behavior in simulation.</li> <li>Deploy the policy on the real humanoid robot.</li> </ul> <h1 id="requirements">Requirements</h1> <ul> <li>Proficient knowledge in programming in Python.</li> <li>Sound understanding of trajectory optimization and reinforcement learning. Students who have taken the Computational Models of Motion course at ETH is a plus but not required.</li> <li>Basic knowledge of the ROS framework is a plus but not required.</li> <li>Since this ambitious project involves the full stack of robotics tools—from motion capture-based data collection, optimization- and learning-based control software, to real-world robot deployment—the student should be devoted, self-driven, and have a strong desire and persistence to explore and solve problems independently.</li> </ul> <h1 id="remarks">Remarks</h1> <p>This is a master thesis project limited to students at ETH Zurich. The project is overseen by Prof. Dr. Stelian Coros and is supervised by Jin Cheng, Dr. Yijiang Huang, and Tianxu An. For further information or to apply for the thesis project, please send your CV, transcript, and a short motivation letter to: Jin (<a href="mailto:jin.cheng@inf.ethz.ch">jin.cheng@inf.ethz.ch</a>), Yijiang (<a href="mailto:yijiang.huang@inf.ethz.ch">yijiang.huang@inf.ethz.ch</a>), and Tianxu (<a href="mailto:tianxu.an@inf.ethz.ch">tianxu.an@inf.ethz.ch</a>).</p> <h1 id="references">References</h1> <p>[1] Lujie Yang, H. J. Terry Suh, Tong Zhao, Bernhard Paus Graesdal, Tarik Kelestemur, Jiuguang Wang, Tao Pang, and Russ Tedrake. Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization, February 2025.</p>]]></content><author><name></name></author><category term="control"/><category term="reinforcement-learning,"/><category term="trajectory-optimization,"/><category term="humanoid,"/><category term="contact-rich-manipulation"/><summary type="html"><![CDATA[Developing demonstration-guided reinforcement learning for complex humanoid manipulation tasks involving strategic contact-rich interactions.]]></summary></entry><entry><title type="html">Robotic motion planning for active environmental manipulation</title><link href="https://crl-ethz.github.io/mp-for-active-env-manip/" rel="alternate" type="text/html" title="Robotic motion planning for active environmental manipulation"/><published>2025-05-22T10:00:00+00:00</published><updated>2025-05-22T10:00:00+00:00</updated><id>https://crl-ethz.github.io/mp-for-active-env-manip</id><content type="html" xml:base="https://crl-ethz.github.io/mp-for-active-env-manip/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Robotic motion planning usually happens in static environments, and we typically do not reason about how we can (or need to) change the environment in order to reach certain places or fulfill certain tasks. In reality, this is an important part that cannot be neglected: think of grabbing a chair to reach an object high up, or building a bridge to walk to another place over a gap (fig. 1). Another setting can be found in the construction industry, where we need to build scaffolding to facilitate work higher up. In these cases, the robot intentionally extends its reachable space by building supports (bridges or makeshift staircases). Traditional motion planning techniques require explicitly representing and enumerating all the different surfaces on which a robot could move, which usually involves case-specific assumptions and engineering heuristics.</p> <p>In this work, we are interested in exploring the motion planning in a changing reachable space that is actively shaped by the robot’s manipulation actions. In order to do so, we want to explore planning using a simulator (such as Isaac Lab or PyBullet) to use physics simulation instead of explicit representation of the surface, etc. The approach we intend to use has close ties to sampling-based kinodynamic motion planning [1, 2].</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025_mp-for-active-env-manip-480.webp 480w,/assets/img/publication_preview/2025_mp-for-active-env-manip-800.webp 800w,/assets/img/publication_preview/2025_mp-for-active-env-manip-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/publication_preview/2025_mp-for-active-env-manip.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Figure 1: Example from Boston Dynamics Atlas robot using a plank to extend its reachable space. Source: <a href="https://www.youtube.com/watch?v=-e1_QhJ1EhQ">Atlas Gets a Grip (Boston Dynamics)</a></em></p> <h1 id="goal">Goal</h1> <p>The ideal outcome of this project is a (task and) motion planning approach that can deal with the requirement to actively change the environment that the robot is in. We will start with a given sequence of actions for the robotic agent (e.g., inspired by the Atlas demo: <em>grab the plank, place the plank in this location</em>) and focus on the motion planning problem.</p> <p>Once we have a solid foundation on motion planning with a fixed action sequence, the project could then evolve in various different directions, e.g., extending this to a multi-robot setting, or focusing more on the task planning aspect in the continuous space.</p> <h1 id="work-packages">Work packages</h1> <ul> <li>Setup scene / explore simulators</li> <li>Setup initial simple problem with simplified dynamics</li> <li>Develop tree-based motion planner</li> </ul> <hr/> <h1 id="requirements">Requirements</h1> <ul> <li>Strong Python knowledge</li> <li>Some background in motion planning is advantageous</li> <li>Previous experience with task and motion planning is advantageous, but not required</li> <li>We prefer self-driven students with a strong desire and persistence to explore and solve problems independently.</li> </ul> <hr/> <h1 id="contact-details">Contact Details</h1> <p>For further information or to apply for this project, please contact:</p> <ul> <li>Yijiang Huang: <a href="mailto:yijiang.huang@inf.ethz.ch">yijiang.huang@inf.ethz.ch</a></li> <li>Valentin Noah Hartmann: <a href="mailto:valentin.hartmann@inf.ethz.ch">valentin.hartmann@inf.ethz.ch</a></li> </ul> <p>We look forward to working with motivated students who are passionate about robotic motion planning. Please include your CV and transcript when reaching out.</p> <hr/> <h1 id="references">References</h1> <p>[1] Dustin J. Webb and Jur van den Berg. Kinodynamic RRT<em>: Asymptotically optimal motion planning for robots with linear dynamics. In *2013 IEEE International Conference on Robotics and Automation</em>, pages 5054–5061, May 2013. doi: 10.1109/ICRA.2013.6631299.</p> <p>[2] Rahul Shome and Lydia E. Kavraki. Asymptotically Optimal Kinodynamic Planning Using Bundles of Edges. In <em>2021 IEEE International Conference on Robotics and Automation (ICRA)</em>, pages 9988–9994, May 2021. doi: 10.1109/ICRA48506.2021.9560836.</p>]]></content><author><name></name></author><category term="planning"/><category term="motion-planning,"/><category term="physics-simulation,"/><category term="mobile-manipulation"/><summary type="html"><![CDATA[inverse design for robotic assembly via task and motion planning (TAMP)]]></summary></entry><entry><title type="html">example</title><link href="https://crl-ethz.github.io/example/" rel="alternate" type="text/html" title="example"/><published>2025-05-20T10:00:00+00:00</published><updated>2025-05-20T10:00:00+00:00</updated><id>https://crl-ethz.github.io/example</id><content type="html" xml:base="https://crl-ethz.github.io/example/"><![CDATA[<p>example</p>]]></content><author><name></name></author><category term="example"/><category term="example"/><summary type="html"><![CDATA[example]]></summary></entry><entry><title type="html">Inverse Design for Robotic Assembly via Task and Motion Planning</title><link href="https://crl-ethz.github.io/inverse-design-for-robotic-assembly/" rel="alternate" type="text/html" title="Inverse Design for Robotic Assembly via Task and Motion Planning"/><published>2025-05-20T10:00:00+00:00</published><updated>2025-05-20T10:00:00+00:00</updated><id>https://crl-ethz.github.io/inverse-design-for-robotic-assembly</id><content type="html" xml:base="https://crl-ethz.github.io/inverse-design-for-robotic-assembly/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Robots are increasingly used to assemble everything from everyday products and mechanical parts to architectural-scale structures. While humans often rely on intuition or heuristics to assess whether a design is buildable, robotic assembly presents a different challenge. The morphological and operational differences between humans and robots mean that human intuition no longer applies — a design that seems feasible to a human may be impossible for a robot to assemble.</p> <p>Although recent advances in robotic manipulation have significantly improved robots’ physical capabilities [1], these systems typically assume a fixed task specification. In contrast, design-for-assembly settings allow for task modifications: if a design proves unassemblable, we want to understand how to alter it to make it feasible for robotic assembly — or even optimize it for robotic efficiency (fig.1).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2025_inverse_design_w_tamp-480.webp 480w,/assets/img/publication_preview/2025_inverse_design_w_tamp-800.webp 800w,/assets/img/publication_preview/2025_inverse_design_w_tamp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/publication_preview/2025_inverse_design_w_tamp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><em>Figure 1: Schematic pipeline of the proposed inverse design for robotic assembly via task and motion planning (TAMP). The design parameters are optimized based on the objectives and constraints defined by the TAMP algorithm.</em></p> <p>This paradigm resembles physics-informed inverse design, where design parameters are optimized using gradients derived from differentiable physics simulations (see, e.g., [2]). However, unlike PDE-based physical simulation, robotic assembly involves complex decision-making over long-horizon, sequential tasks — falling within the scope of Task and Motion Planning (TAMP) [3], which jointly reasons over “what to do” and “how to do it.”</p> <p>Our goal is to develop a design optimization algorithm that optimizes a parameterized assembly design with respect to objectives and constraints defined by a TAMP algorithm. For simplicity, we will restrict the design domain to a set of trusses, parameterized by their geometry, topology, or learned latent variables. We will couple a search-based task planner with an optimization-based grasp and motion planner as part of the TAMP stack.</p> <h1 id="tasks">Tasks</h1> <ul> <li>Implement a search-based task planner to identify feasible assembly sequences, conditioned on motion constraints [4].</li> <li>Develop an optimization-based grasp and inverse kinematics planner with extra design parameters for the parametrized design, e.g., [5].</li> <li>Validate the TAMP pipeline (“forward pass”) on a fixed design.</li> <li>Extract gradients from the motion planning components to update design parameters.</li> <li>Study how changes in design affect the discrete task planner and analyze implications for gradient continuity and optimization stability.</li> </ul> <h1 id="requirements">Requirements</h1> <ul> <li>Strong interest in robotics, particularly in task and motion planning.</li> <li>Familiarity with optimization-based motion planning (e.g., trajectory optimization) is a plus.</li> <li>Proficiency in Python.</li> <li>Experience with robot simulation environments (e.g., PyBullet) is preferred but not required.</li> </ul> <h1 id="remarks">Remarks</h1> <p>This is a master thesis opportunity. The thesis is overseen by Prof. Dr. Stelian Coros and is supervised by Dr. Yijiang Huang and Dr. Gabriele Fadini. For further information or to apply for the thesis project, please send your CV, transcript, and a short motivation letter to: Yijiang (<a href="mailto:yijiang.huang@inf.ethz.ch">yijiang.huang@inf.ethz.ch</a>) and Gabriele (<a href="mailto:gabfadini@gmail.com">gabfadini@gmail.com</a>).</p> <h1 id="references">References</h1> <ol> <li> <p>Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. <em>π0: A vision-language-action flow model for general robot control.</em> arXiv preprint <a href="https://arxiv.org/abs/2410.24164">arXiv:2410.24164</a>, 2024.</p> </li> <li> <p>Juan Sebastian Montes Maestre, Yinwei Du, Ronan Hinchet, Stelian Coros, and Bernhard Thomaszewski. <em>Differentiable stripe patterns for inverse design of structured surfaces.</em> ACM Transactions on Graphics (TOG), 42(4):1–14, 2023.</p> </li> <li> <p>Caelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim, Tom Silver, Leslie Pack Kaelbling, and Tomás Lozano-Pérez. <em>Integrated task and motion planning.</em> Annual Review of Control, Robotics, and Autonomous Systems, 4(1):265–293, 2021.</p> </li> <li> <p>Yijiang Huang, Caelan R. Garrett, Ian Ting, Stefana Parascho, and Caitlin T. Mueller. <em>Robotic additive construction of bar structures: Unified sequence and motion planning.</em> Construction Robotics, 5(2):115–130, June 2021. DOI: <a href="https://doi.org/10.1007/s41693-021-00062-z">10.1007/s41693-021-00062-z</a>.</p> </li> <li> <p>Simon Zimmermann, Ghazal Hakimifard, Miguel Zamora, Roi Poranne, and Stelian Coros. <em>A multi-level optimization framework for simultaneous grasping and motion planning.</em> IEEE Robotics and Automation Letters, 5(2):2966–2972, 2020.</p> </li> </ol>]]></content><author><name></name></author><category term="planning"/><category term="task-and-motion-planning,"/><category term="robotic-assembly,"/><category term="inverse-design,"/><category term="co-design"/><summary type="html"><![CDATA[inverse design for robotic assembly via task and motion planning (TAMP)]]></summary></entry></feed>